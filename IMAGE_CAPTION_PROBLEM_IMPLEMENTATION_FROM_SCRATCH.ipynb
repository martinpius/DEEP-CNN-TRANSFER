{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMAGE CAPTION PROBLEM-IMPLEMENTATION FROM SCRATCH",
      "provenance": [],
      "authorship_tag": "ABX9TyNfK+nJf5IWHJbsznfFAbEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/DEEP-CNN-TRANSFER/blob/main/IMAGE_CAPTION_PROBLEM_IMPLEMENTATION_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1hERmZqwzz8",
        "outputId": "214766b1-bbc5-419b-8edc-04f88ecb5b10"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\"You are on CoLaB with torch version {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)}: {e}\\n>>>> please correct {type(e)} and reload your drive\")\n",
        "  COLAB = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "def time_fmt(t: float = 231.21)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"h: {h} min: {m:>02} sec: {s:>05.2f}\"\n",
        "print(f\">>>> testing the time formating function...\\n>>>> time elapsed\\t{time_fmt()}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are on CoLaB with torch version 1.9.0+cu102\n",
            ">>>> testing the time formating function...\n",
            ">>>> time elapsed\th: 0 min: 03 sec: 51.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWDhlj4zyo7D"
      },
      "source": [
        "# In this network we are going to implement a neural network for image caption problem.\n",
        "# The network is going to take an image captioned by a text which discribes what is on the picture\n",
        "# This network combine two srchitectures, the CNN (which extract representative details) of the images\n",
        "# and the LSTM which predict the image discription based on the information received from the features\n",
        "# The model is a typical kind of encoder- decoder. The encoder being the CNN and decoder the LSTM\n",
        "# we also add an attention mechanism to learn important representation from the images to improve prediction"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrqkBbEez6zZ",
        "outputId": "53026a42-7b2b-42b5-9f38-601f431dca1d"
      },
      "source": [
        "import torch, spacy, os\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from math import ceil\n",
        "import random, time, datetime\n",
        "import numpy as np\n",
        "from tensorflow import summary\n",
        "import pandas as pd\n",
        "%load_ext tensorboard\n",
        "spacy_eng = spacy.load('en')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc5UeJsO1TTD"
      },
      "source": [
        "#set the random seed for reproducability and the gpu to deterministic:\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTOqqM1x13HI"
      },
      "source": [
        "# We start with an encoder class: This is a CNN, we prefer to use transfer learning\n",
        "# approach (load the pre-trained model), for our case we will use Inception-v3 network\n",
        "# we will remove the classifier and replace the final layer before the classifier with\n",
        "# an embedding layer (final output of the encoder must correspons to an embedding dim)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSBDntaS2hiM"
      },
      "source": [
        "\n",
        "class CNNENC(nn.Module):\n",
        "  def __init__(self, embedding_dim, train = False):\n",
        "    super(CNNENC, self).__init__()\n",
        "    self.train = train\n",
        "    self.inception_v3 = torchvision.models.inception_v3(pretrained = True, aux_logits = False)\n",
        "    self.inception_v3.fc = nn.Linear(self.inception_v3.fc.in_features, embedding_dim)\n",
        "    self.drp = nn.Dropout(p = 0.5)\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    # we first grab the features of an image\n",
        "    features = self.inception_v3(input_tensor)\n",
        "    # we are training only the last layer of the inception network\n",
        "    for name, param in self.inception_v3.named_parameters():\n",
        "      if \"fc_weight\" in name or \"fc_bias\" in name:\n",
        "        param.require_grad = True\n",
        "      else:\n",
        "        param.require_grad = self.train\n",
        "    return self.drp(self.relu(features))\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "539G8NDA8SFf"
      },
      "source": [
        "# the decorder class is the ussual RNN with an LSTM architecture\n",
        "# the input to this network is the feature vectors from the encoder's network\n",
        "# the dimension of the feature vectors must be equal to an embedding dim of LSTM\n",
        "class LSTMDEC(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size,num_layers):\n",
        "    super(LSTMDEC, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(input_size = embedding_dim, \n",
        "                        hidden_size = hidden_dim, \n",
        "                        num_layers = num_layers,\n",
        "                        dropout = 0.5)\n",
        "    self.drp = nn.Dropout(p = 0.5)\n",
        "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "  \n",
        "  def forward(self, features, captions):\n",
        "    ''' \n",
        "    the foward will pass in features from cnn-decoder and the captions (texts)\n",
        "    to return the output which is the caption\n",
        "\n",
        "    '''\n",
        "    # first get the embedding for the target (captions)\n",
        "    embeddings = self.drp(self.embed(captions))\n",
        "    # add a batch dimension to the features and concatenate with the embeddings\n",
        "    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim = 0)\n",
        "    # input to lstm include both features from images (pixels and the embeded texts from captions)\n",
        "    hiddens, _ = self.lstm(embeddings)\n",
        "    outputs = self.fc(hiddens) # get the output of our decorder\n",
        "    return outputs\n",
        "\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwCiyD32CVxH"
      },
      "source": [
        "# We now build the model class which combines both the encoder and the decorder\n",
        "class AUTOENC(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers):\n",
        "    super(AUTOENC, self).__init__()\n",
        "    self.encoder = CNNENC(embedding_dim = embedding_dim)\n",
        "    self.decoder = LSTMDEC(embedding_dim, hidden_dim,vocab_size, num_layers)\n",
        "  \n",
        "  def forward(self, images, captions):\n",
        "    features = self.encoder(images) # run the encoder (CNN)\n",
        "    outputs = self.decoder(images, captions) # run the decoder (the LSTM)\n",
        "    return outputs\n",
        "\n",
        "  def pred_captions(self, images, vocabulary, max_len = 50):\n",
        "    '''\n",
        "    Here we receives images without any caption. We use our\n",
        "    model to predict what will be the probable caption. We fix length of the \n",
        "    caption to 50 words. We also provide vocabulary for the network to choose mostly\n",
        "    likely words combinations based on the image. The idea is like to predict the mostly\n",
        "    likely next word in a sentence.\n",
        "\n",
        "    '''\n",
        "    caption_res = [] # container for our caption (predicted)\n",
        "    # no need to train the network again\n",
        "    with torch.no_grad():\n",
        "      feature = CNNENC(images).unsqueeze(0) # get the features\n",
        "      states = None # we initialize the  lstm states to none [directly will be zeros at start]\n",
        "      for _ in range(max_len):\n",
        "        hidden, states = self.decoder.lstm(feature, states)\n",
        "        output = self.decoder.fc(hidden.squeeze(0))\n",
        "        pred_word = output.argmax(1) # grab the maximum probability in a (class probs = vocab_size)\n",
        "        caption_res.append(pred_word.item()) # append the mostly probable word in a sentence.\n",
        "        # the next input to the LSTM will be the output of the previous step\n",
        "        feature = self.decoder.embed(pred_word).unsqueeze()\n",
        "        # stop / quit the loop when we reach end of sentence\n",
        "        if vocabulary.itos[pred_word.item()] == \"<EOS>\":\n",
        "          break\n",
        "        # we now return our predicted caption. \n",
        "    return [vocabulary.itos[idx] for idx in caption_res]\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUTHDPXYitrL"
      },
      "source": [
        "# A class to build vocabulary\n",
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    '''\n",
        "    We construct a dictionary which key-values as index-tring and vice-versa\n",
        "    to convert the strings to indice and indices back to strings\n",
        "    <UNK> is when the word doesnt bit the frequency threshold limit.\n",
        "    '''\n",
        "    self.freq_threshold = freq_threshold\n",
        "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod\n",
        "  def eng_tokenizer(text):\n",
        "    ''' \n",
        "    we use spacy-tokenizer to tokenize the texts and then change them to lower cases\n",
        "    '''\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "  \n",
        "  def build_vocabulary(self, caption_list):\n",
        "    '''\n",
        "    we send in caption list and build a corpus / bag of vocabulary\n",
        "    in every caption we inspect the each word count. If the word occured\n",
        "    more than frequency threshold we assign an index otherwise it will be assigned\n",
        "    to unknown index.\n",
        "    '''\n",
        "    frequencies = {} # a dictionary / place-holder to store the words\n",
        "    idx = 4 # we start at 4 because 0 = PAD, 1 = SOS, 2 = EOS, 3 = UNK\n",
        "    for caption in caption_list:\n",
        "      for word in caption:\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "\n",
        "        #here we do the conversion if the criteria is met\n",
        "        if frequencies[word] == self.freq_threshold:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    '''\n",
        "    we actually convert the texts to numerics using this method\n",
        "\n",
        "    '''\n",
        "    tokenized_text = self.eng_tokenizer(text) # get the tokens in lower cases\n",
        "    return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi['<UNK>']\n",
        "            for token in tokenized_text\n",
        "    ]\n",
        "      \n",
        "\n",
        "# a class to Load the data from google drive\n",
        "class Flickr30kData(Dataset):\n",
        "  def __init__(self, root_dir, csv_file, transform = None, freq_threshold = 5):\n",
        "    '''\n",
        "    root_dir == directory to images folder\n",
        "    csv_file == csv file directory for image discription (id, caption)\n",
        "    transform == if we will apply some transformations to images\n",
        "    freq_threshold == frequency threshold to keep most frequent words in captions\n",
        "\n",
        "    '''\n",
        "    self.root_dir = root_dir\n",
        "    self.dfm = pd.read_csv(csv_file, error_bad_lines = False) # we read the csv file from a specified directory\n",
        "    self.transform = transform\n",
        "\n",
        "    # Grab the image id and caption data from the panda dataframe:\n",
        "    self.imgs = self.dfm['image']\n",
        "    self.captions = self.dfm['caption']\n",
        "\n",
        "    #initialize and buil a vocabulary\n",
        "    self.vocab = Vocabulary(freq_threshold) # we use Vocabulary class (to be defined)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "  \n",
        "  def __len__(self):\n",
        "    ''' \n",
        "    we grasp total number of examples from our data frame to mark the end of our\n",
        "    loop when we load one datapoint after the other\n",
        "\n",
        "    '''\n",
        "    return len(self.dfm)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    this method help to grasp one sample at a time\n",
        "    a single image with a corresponding caption\n",
        "\n",
        "    '''\n",
        "    caption = self.captions[idx] # grab a caption from an image (texts) from image description csv_file\n",
        "    img_id = self.imgs[idx] # grab the image id from an image description csv-file\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\") # grab an image from the image folder and convert to RGB\n",
        "    # we apply some transformation to the image if needed\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    \n",
        "    # We pre-process the texts here: captions (change into numeric)\n",
        "    numericalized_caption = [self.vocab.stoi['<SOS>']] # we start at the begining of the sentennce (SOS)\n",
        "    numericalized_caption += self.vocab.numericalize(caption) # change the caption to numeric\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"]) # mark the end of the sentence <EOS>\n",
        "    # convert to a tensor and then return the image with the corresponding caption\n",
        "    return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "# Since every caption is of specified legth, economically we need to pad-the generated \n",
        "# sequences with the maximum length of a sentence on a specified batch.\n",
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx \n",
        "\n",
        "  def __call__(self, batch):\n",
        "    images = [item[0].unsqueeze(0) for item in batch] # list of images with an added batch dimension\n",
        "    images = torch.cat(images, dim = 0) # combine images accross the batch dims\n",
        "    targets = [item[1] for item in batch] # grab all captions \n",
        "    targets = pad_sequence(targets, batch_first = False,padding_value = self.pad_idx) # pad every batch with its max len\n",
        "    return images, targets\n",
        "\n",
        "# We finally define our iterator (dataloader method to stream the data during training)\n",
        "def get_loader(images_dir,\n",
        "               csv_dir,\n",
        "               transform,\n",
        "               batch_size = 64,\n",
        "               shuffle = True,\n",
        "               pin_memory = True):\n",
        "  \n",
        "  #instantiate the data-loader, splits the data into batches padded independntly with their max len\n",
        "  my_flickrdata = Flickr30kData(images_dir, csv_dir, transform)\n",
        "  pad_idx = my_flickrdata.vocab.stoi[\"<PAD>\"] # to use in the custom- collate function\n",
        "  loader = DataLoader(dataset = my_flickrdata, \n",
        "                      batch_size = batch_size, \n",
        "                      shuffle = shuffle,\n",
        "                      pin_memory = pin_memory, \n",
        "                      collate_fn = MyCollate(pad_idx = pad_idx))\n",
        "  return loader, my_flickrdata"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz-MnmqfZx6E"
      },
      "source": [
        "\n",
        "# We now implementing the function to train the above network\n",
        "def train():\n",
        "  transform = transforms.Compose([\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Resize((356,356)),\n",
        "                                  transforms.RandomCrop((299, 299)),\n",
        "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "  ]) \n",
        "\n",
        "\n",
        "  loader, dataset = get_loader(images_dir = \"/content/drive/MyDrive/flickr30k_images/flickr8k/images\", \n",
        "                               csv_dir = \"/content/drive/MyDrive/flickr30k_images/flickr8k/captions.txt\", \n",
        "                               transform = transform)\n",
        " \n",
        "  # Hyper-parameters\n",
        "  learning_rate = 1e-3\n",
        "  EPOCHS = 10\n",
        "  num_layers = 2\n",
        "  embedding_size = 300\n",
        "  hidden_dim =  512\n",
        "  vocab_size = len(dataset.vocab)\n",
        "  \n",
        "  # tensorboard environment\n",
        "  curr_time = datetime.datetime.now().timestamp()\n",
        "  my_dir = \"logs/tensorboard/image_captions\" + str(curr_time)\n",
        "  writer = summary.create_file_writer(my_dir)\n",
        "  step = 0\n",
        "  \n",
        "  # Model's initialization\n",
        "  model = AUTOENC(embedding_dim = embedding_size,\n",
        "                  hidden_dim = hidden_dim,\n",
        "                  vocab_size = vocab_size, \n",
        "                  num_layers = num_layers).to(device = device)\n",
        "  optimizer = optim.RMSprop(params = model.parameters(), lr = learning_rate)\n",
        "  loss_obj = nn.CrossEntropyLoss(ignore_index = dataset.vocab.stoi['<PAD>'])\n",
        "\n",
        "  # the train loop\n",
        "  tic = time.time()\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"\\n>>>> train starts for epoch {epoch + 1}\\n>>>> please wait while the model is training................\")\n",
        "    for idx, (image, caption) in enumerate(tqdm(loader)):\n",
        "      image = image.to(device = device)\n",
        "      caption = caption.to(device = device)\n",
        "      pred = model(image, caption[:-1]) # we want the model to predict EOS token that why we do not send it in\n",
        "      loss = loss_obj(pred.reshape(-1, pred.shape[2]), caption.reshape(-1))\n",
        "      with writer.as_default():\n",
        "        summary.scalar(\"training_loss\", loss.item(), step = step)\n",
        "        step+=1\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(loss)\n",
        "      optimizer.step()\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdU48Y0t1ZXK",
        "outputId": "06843e88-bae7-46c5-efae-fe6e90611f64"
      },
      "source": [
        "train()\n",
        "toc = time.time()\n",
        "print(f\"\\n>>>> time elapsed: {time_fmt(toc - tic)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/633 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> train starts for epoch 1\n",
            ">>>> please wait while the model is training................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbVnczh92tIO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}